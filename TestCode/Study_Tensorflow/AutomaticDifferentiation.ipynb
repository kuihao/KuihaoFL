{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cca3b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "https://www.tensorflow.org/guide/autodiff\n",
    "https://www.twblogs.net/a/5b7b88322b71770a43d8bccf\n",
    "自動微分法是一種介於符號微分和數值微分的方法：\n",
    "數值微分強調一開始直接代入數值近似求解；\n",
    "符號微分強調直接對代數進行求解，最後才代入問題數值；\n",
    "自動微分將符號微分法應用於最基本的算子，\n",
    "比如常數，冪函數，指數函數，對數函數，三角函數等，然後代入數值，\n",
    "保留中間結果，最後再應用於整個函數。\n",
    "'''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "778adc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "460c8b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TensorFlow \"records\" relevant operations executed inside \n",
    "the context of a tf.GradientTape onto a \"tape\". \n",
    "TensorFlow then uses that tape to compute the gradients of \n",
    "a \"recorded\" computation using reverse mode differentiation.\n",
    "'''\n",
    "x = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e74807a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dy = 2x * dx\n",
    "dy_dx = tape.gradient(y, x)\n",
    "dy_dx.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4444fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[1. 3.]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(2.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "w = tf.Variable([[1.,0.],\n",
    "                 [0.,0.],\n",
    "                 [0.,1.]], name='w') #(tf.random.normal((3, 2)), name='w')\n",
    "b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
    "x = [[1., 2., 3.]]\n",
    "\n",
    "loss_fn_mse = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# 將操作tensor的運算式紀錄於GradientTape，後續可以用tape對特定運算式對特定變數計算導數\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y = x@w + b # 應該是 tensor 內積? 維度要對\n",
    "    print(y)\n",
    "    loss_value = loss_fn_mse(y_true=3, y_pred=y) # mu((y_true - y_pred)^2)\n",
    "    print(loss_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50f1d7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'w:0' shape=(3, 2) dtype=float32, numpy=\n",
      "array([[1., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 1.]], dtype=float32)> (3, 2)\n",
      "tf.Tensor(\n",
      "[[-2.  0.]\n",
      " [-4.  0.]\n",
      " [-6.  0.]], shape=(3, 2), dtype=float32) (3, 2)\n"
     ]
    }
   ],
   "source": [
    "[dl_dw, dl_db] = tape.gradient(loss_value, [w, b])\n",
    "print(w, w.shape)\n",
    "print(dl_dw, dl_dw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da9915b",
   "metadata": {},
   "source": [
    "# 畫圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a9566a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加上@tf.function裝飾器，可使返回的 tf struct 加進靜態圖，並返回導數結果\n",
    "\n",
    "@tf.function # Compiles a function into a callable TensorFlow graph\n",
    "def SetIntoGraph(x):\n",
    "    return tf.matmul(x,tf.transpose(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4fa3b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[14.]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Set up logging.\n",
    "stamp = time.strftime(\"%Y%m%d-%H%M%S\", time.localtime())\n",
    "logdir = \"logs/func/%s\" % stamp\n",
    "writer = tf.summary.create_file_writer(logdir)\n",
    "\n",
    "w = tf.Variable([[1.,0.],\n",
    "                 [0.,0.],\n",
    "                 [0.,1.]], name='w') #(tf.random.normal((3, 2)), name='w')\n",
    "b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
    "x = [[1., 2., 3.]]\n",
    "\n",
    "loss_fn_mse = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "tf.summary.trace_on(graph=True)\n",
    "tf.profiler.experimental.start(logdir)\n",
    "# Call only one tf.function when tracing.\n",
    "z = print(SetIntoGraph(x))\n",
    "with writer.as_default():\n",
    "    tf.summary.trace_export(\n",
    "        name=\"my_func_trace\",\n",
    "        step=0,\n",
    "        profiler_outdir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "105cb9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cmd input: tensorboard --logdir logs\\func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d0fbd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-cuda115",
   "language": "python",
   "name": "tf-gpu-cuda115"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
