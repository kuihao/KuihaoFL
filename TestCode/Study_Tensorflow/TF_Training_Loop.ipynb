{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0b27034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'使用裝飾器@tf.function、keras api，皆是使用靜態圖，運算效能高\\n使用 tf.GradientTape() 則是使用動態圖，效能較低\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "使用裝飾器@tf.function、keras api，皆是使用靜態圖，運算效能高\n",
    "純粹使用 tf.GradientTape() 則是使用動態圖，效能較低\n",
    "tf.GradientTape 是自動微分 (Record operations for automatic differentiation.)\n",
    "tf.gradients(ys, xs, ...) 是符號微分 (Constructs symbolic derivatives of sum of ys w.r.t. x in xs.)\n",
    "'''\n",
    "'''靜態/動態圖、符號/自動微分'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81891969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'名詞翻譯'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "名詞翻譯: \n",
    "dimension reduction: 維度縮減 https://terms.naer.edu.tw/detail/3648035/?index=1\n",
    "Hadamard product: 哈德瑪得乘積 (定義兩同維度矩陣，相應元素計算乘積)，\n",
    "又稱 element-wise product(逐元乘積)、entrywise product(逐項乘積)\n",
    "---\n",
    "reduce(縮減) \n",
    "entrywise product(逐項乘積)\n",
    "---\n",
    "tf.matmul() 兩張量(矩陣)相乘\n",
    "tf.multiply() 兩張量(矩陣)逐項乘積\n",
    "tf.reduce_sum() \n",
    "對張量(矩陣)指定維度的元素進行相加(omputes the sum of elements across dimensions of a tensor.)\n",
    "numpy.ufunc.reduce() \n",
    "對同一維度的元素套用相同的操作，將陣列的維度縮減(較少翻譯為:歸約)至1維 (Reduces array’s dimension by one, by applying ufunc along one axis.)\n",
    "tf.gradients(ys, xs, ...)\n",
    "Constructs symbolic derivatives (符號導數) of sum of ys w.r.t. x in xs.\n",
    "symbolic differentiation 符號微分法\n",
    "'''\n",
    "'名詞翻譯'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b52a5c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Derivative'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "https://stackoverflow.com/questions/43455320/difference-between-symbolic-differentiation-and-automatic-differentiation\n",
    "\n",
    "There are 3 popular methods to calculate the derivative:\n",
    "1. Numerical differentiation: 數值方法，定義合理的方程式，透過多次迭代來減少誤差項，逼近理論解析解\n",
    "2. Symbolic differentiation: 透過連鎖律獲得導函數表達式，計算微分值\n",
    "3. Automatic differentiation:  Automatic differentiation is the same as \n",
    "   Symbolic differentiation (in one place they operate on math expression, \n",
    "   in another on computer programs). And yes, they are sometimes very similar. \n",
    "   But for control flow statements (`if, while, loops) the results can be very different:\n",
    "   symbolic differentiation leads to inefficient code (unless carefully done) and faces the difficulty of converting a computer program into a single expression\n",
    "'''\n",
    "'''Derivative'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a60c0313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TF Eager Execution'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "https://ithelp.ithome.com.tw/articles/10217112\n",
    "https://ithelp.ithome.com.tw/articles/10216085\n",
    "https://pytorch.org/tutorials/beginner/examples_autograd/tf_two_layer_net.html\n",
    "最初 Tensorflow 是以靜態計算圖（static computational graph）的方式進行 gradient 計算\n",
    "Tensorflow 制定的 tensor 結構可以放置 CPU 或 GPU，而 numpy ndarray 指能以 CPU 計算\n",
    "故 Tensorflow 可將 ndarray 轉換至 tensor、tf.graph 也可放置於 GPU\n",
    "\n",
    "相較於 Pytorch 的動態計算圖（dynamic computational graph）會在執行期間\n",
    "自動微分(Runtime Automatic Differentation)；\n",
    "TF 也是自動微分\n",
    "\n",
    "大約從 TF 1.5 版開始推出 Eager Execution，Eager_tensor 就是可動態圖的方法\n",
    "'''\n",
    "'''TF Eager Execution'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76d73140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e83a31e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:06:23\n"
     ]
    }
   ],
   "source": [
    "current_time_str = time.strftime(\"%H:%M:%S\", time.localtime())\n",
    "print(current_time_str)\n",
    "#print(type(current_time_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "104e9a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the training dataset.\n",
    "batch_size = 64\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = np.reshape(x_train, (-1, 784))\n",
    "x_test = np.reshape(x_test, (-1, 784))\n",
    "\n",
    "# Reserve 10,000 samples for validation.\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]\n",
    "\n",
    "# Prepare the training dataset.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "# Prepare the validation dataset.\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c73127ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model\n",
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
    "outputs = layers.Dense(10, name=\"predictions\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Instantiate an optimizer to train the model.\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "# Instantiate a loss function.\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Prepare the metrics.\n",
    "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = keras.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9d9546",
   "metadata": {},
   "source": [
    "# Eager mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba14e1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 102.7714\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 1.4990\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 1.2216\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 0.9557\n",
      "Seen so far: 38464 samples\n",
      "Training acc over epoch: 0.6387\n",
      "Validation acc: 0.7874\n",
      "Time taken: 3.95s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 1.1476\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 0.9257\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 0.7298\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 0.7865\n",
      "Seen so far: 38464 samples\n",
      "Training acc over epoch: 0.8113\n",
      "Validation acc: 0.8509\n",
      "Time taken: 3.75s\n"
     ]
    }
   ],
   "source": [
    "# *** 第一步: 設定迭代次數 *** \n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    start_time = time.time()\n",
    "\n",
    "    # *** 第二步: 取出 batches 的資料，\n",
    "    #            向前傳播紀錄權重運算結果、得到預測結果，(預設)計算平均loss值，\n",
    "    #            此時可再加入 loss 的正規化\n",
    "    #            再計算Loss_fn對權重的導數(自動微分: 先前紀錄所有運算過程，再反向傳播運算得微分值)\n",
    "    #            最後用最佳化器更新權重\n",
    "    # *** \n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forwarding\n",
    "            pred_logits = model(x_batch_train, training=True)\n",
    "            # loss_object = loos_fn(y_true, y_pred)\n",
    "            # `loss_object` can get loss value and regards as a function can compute gradient \n",
    "            loss_object = loss_fn(y_batch_train, pred_logits)\n",
    "        grads = tape.gradient(loss_object, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Update training metric.\n",
    "        train_acc_metric.update_state(y_batch_train, pred_logits)\n",
    "\n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_object))\n",
    "            )\n",
    "            print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n",
    "\n",
    "    # Display metrics at the end of each epoch.\n",
    "    train_acc = train_acc_metric.result()\n",
    "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "\n",
    "    # Reset training metrics at the end of each epoch\n",
    "    train_acc_metric.reset_states()\n",
    "\n",
    "    # Run a validation loop at the end of each epoch.\n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        val_logits = model(x_batch_val, training=False)\n",
    "        # Update val metrics\n",
    "        val_acc_metric.update_state(y_batch_val, val_logits)\n",
    "    val_acc = val_acc_metric.result()\n",
    "    val_acc_metric.reset_states()\n",
    "    print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "    print(\"Time taken: %.2fs\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecdac49",
   "metadata": {},
   "source": [
    "# Static mode?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c6c143",
   "metadata": {},
   "source": [
    "## Appetizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9a9faab",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def MyStaticGradientor(func, var):\n",
    "    return func(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db8d4f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = tf.constant([[2,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25e238d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_fn(x):\n",
    "    res = x**3\n",
    "    print(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c50cc9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[8 8]], shape=(1, 2), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[8, 8]])>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "square_fn(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc5b383b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"pow:0\", shape=(1, 2), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[8, 8]])>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyStaticGradientor(square_fn, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792de7f1",
   "metadata": {},
   "source": [
    "## Main course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b28c52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_efficient = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebb4e5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred_logits = model_efficient(x, training=True)\n",
    "        loss_value = loss_fn(y, pred_logits)\n",
    "    grads = tape.gradient(loss_value, model_efficient.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model_efficient.trainable_weights))\n",
    "    train_acc_metric.update_state(y, pred_logits)\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e6bec85",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(x, y):\n",
    "    pred_logits = model_efficient(x, training=False)\n",
    "    val_acc_metric.update_state(y, pred_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fda90e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 0.9626\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 0.3760\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 0.5383\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 0.4413\n",
      "Seen so far: 38464 samples\n",
      "Training acc over epoch: 0.8542\n",
      "Validation acc: 0.8769\n",
      "Time taken: 1.11s\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 0.8899\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 0.4732\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 0.2711\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 0.6874\n",
      "Seen so far: 38464 samples\n",
      "Training acc over epoch: 0.8796\n",
      "Validation acc: 0.8410\n",
      "Time taken: 0.70s\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Start of epoch {epoch}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # ---- Training ----\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        loss_value = train_step(x_batch_train, y_batch_train)\n",
    "\n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n",
    "\n",
    "    # Display metrics at the end of each epoch.\n",
    "    train_acc = train_acc_metric.result()\n",
    "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "    # Reset training metrics at the end of each epoch\n",
    "    train_acc_metric.reset_states()\n",
    "\n",
    "    # ---- Testing (Validating) ----\n",
    "    # Run a validation loop at the end of each epoch.\n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        test_step(x_batch_val, y_batch_val)\n",
    "\n",
    "    val_acc = val_acc_metric.result()\n",
    "    val_acc_metric.reset_states()\n",
    "    print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "    \n",
    "    print(\"Time taken: %.2fs\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce726fee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8103e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e142fba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349fface",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-cuda115",
   "language": "python",
   "name": "tf-gpu-cuda115"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
